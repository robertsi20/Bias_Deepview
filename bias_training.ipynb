{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99524adf-d5a9-4e5e-a0c7-d25c7e8657d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from nlp_helper_scripts.getData import (prepare_label, bert_glue_encode,pretrained_bert_model)\n",
    "from bias_helpers.bias_functions import tprs, calculate_gaps\n",
    "from bias_helpers.bias_deepview import DeepViewBias\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "MODEL_CHECKPOINT = \"bert-base-uncased\"\n",
    "task = \"bias_in_bios\"\n",
    "def single_preprocess_function(examples):\n",
    "    # glue datasets field mapping\n",
    "    task_to_keys = {\n",
    "        \"cola\": (\"sentence\", None),\n",
    "        \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "        \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "        \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "        \"qnli\": (\"question\", \"sentence\"),\n",
    "        \"qqp\": (\"question1\", \"question2\"),\n",
    "        \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "        \"sst2\": (\"sentence\", None),\n",
    "        \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "        \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "        \"bias_in_bios\": (\"hard_text\", None),\n",
    "    }\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    sentence1_key, sentence2_key = task_to_keys[task]\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key],\n",
    "                         max_length=128,\n",
    "                         padding='max_length',\n",
    "                         truncation=True)\n",
    "    return tokenizer(examples[sentence1_key],\n",
    "                     examples[sentence2_key],\n",
    "                     max_length=128,\n",
    "                     padding='max_length',\n",
    "                     truncation=True)\n",
    "\n",
    "preprocessed_dataset = dataset.map(single_preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cec89e-35aa-4a32-8d9a-03b9214be937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bert_glue_encode(dataset):\n",
    "    # Convert batch of encoded features to numpy array.\n",
    "    input_ids = np.array(dataset[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(dataset[\"attention_mask\"], dtype=\"int32\")\n",
    "    token_type_ids = np.array(dataset[\"token_type_ids\"], dtype=\"int32\")\n",
    "    labels = np.array(dataset[\"profession\"], dtype=\"int32\")\n",
    "    \n",
    "    #add check for test set since they may not have labels\n",
    "    return (input_ids, attention_masks, token_type_ids) ,labels\n",
    "\n",
    "x_train, y_train = bert_glue_encode(preprocessed_dataset['train'])\n",
    "x_val, y_val = bert_glue_encode(preprocessed_dataset['test'])\n",
    "\n",
    "model_y_train = prepare_label(y_train)\n",
    "model_y_val = prepare_label(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966bb2b-4257-4bc5-8621-26770fdc1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getData import pretrained_bert_model, prepare_dataset, classifier_model, finetuned_bert_and_classifier\n",
    "import tensorflow as tf\n",
    "\n",
    "task = 'bias_in_bios'\n",
    "# %matplotlib qt\n",
    "\n",
    "pt_embed = pretrained_bert_model()\n",
    "pt_head = classifier_model(28)\n",
    "whole_model, ft_bert, ft_classifier = finetuned_bert_and_classifier(28)\n",
    "\n",
    "\n",
    "#This will take a while. maybe grab a coffee or a broetchen something?\n",
    "train_embeddings = pt_embed.predict(x_train, batch_size=64)\n",
    "val_embeddings  = pt_embed.predict(x_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83250f78-fde5-49d8-baa6-a124bae9793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "num_classes = 28\n",
    "if num_classes == 2:\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "else:\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    metrics = tf.metrics.CategoricalAccuracy()\n",
    "\n",
    "\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.int32)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "\n",
    "#recommend 25-40 epochs on the classifier and maybe 3 on all of BERT. \n",
    "\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "\n",
    "steps_per_epoch = len(x_train[1])\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=num_warmup_steps, num_train_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301a8d8-2a65-42d8-887c-5d97ea021e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_head.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "_ = pt_head.fit(train_embeddings, model_y_train, validation_data=(val_embeddings, model_y_val), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec40ad-089f-4b70-a6b5-4a241fb6195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "_ = whole_model.fit(x_train, model_y_train, validation_data=(x_val, model_y_val), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c07e59-f358-457c-8e97-c188b79b96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_pretrained_classifier_only_path = './models/' + str(task)+ '/{}_pretrained_BERT_Classifier'.format(task.replace('/', '_'))\n",
    "\n",
    "saved_finetuned_whole_model_path = './models/' + str(task)+ '/{}_finetuned_BERT'.format(task.replace('/', '_'))\n",
    "saved_finetuned_embed_model_path = './models/' + str(task)+ '/{}_finetuned_BERT_Embeddings'.format(task.replace('/', '_'))\n",
    "saved_finetuned_predict_model_path = './models/' + str(task)+ '/{}_finetuned_BERT_Predictor'.format(task.replace('/', '_'))\n",
    "\n",
    "\n",
    "# pt_head.save(saved_pretrained_classifier_only_path, include_optimizer=False)\n",
    "# whole_model.save(saved_finetuned_whole_model_path, include_optimizer=False)\n",
    "ft_bert.save(saved_finetuned_embed_model_path, include_optimizer=False)\n",
    "ft_classifier.save(saved_finetuned_predict_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211d291-720b-4bc7-adfb-136651572abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
